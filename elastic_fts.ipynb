{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrian/anaconda3/envs/nlp-labs/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  _id title                                               text\n",
      "0   3        Nie mówię, że nie podoba mi się też pomysł szk...\n",
      "1  31        Tak więc nic nie zapobiega fałszywym ocenom po...\n",
      "2  56        Nigdy nie możesz korzystać z FSA dla indywidua...\n",
      "3  59        Samsung stworzył LCD i inne technologie płaski...\n",
      "4  63        Oto wymagania SEC: Federalne przepisy dotycząc...\n",
      "{'_id': '0', 'title': '', 'text': 'Co jest uważane za wydatek służbowy w podróży służbowej?'}\n",
      "   query-id  corpus-id  score\n",
      "0         8     566392      1\n",
      "1         8      65404      1\n",
      "2        15     325273      1\n",
      "3        18      88124      1\n",
      "4        26     285255      1\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "df_corpus = pd.DataFrame(ds['corpus'])\n",
    "print(df_corpus.head())\n",
    "\n",
    "ds_q = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "ds_q = ds_q['queries']\n",
    "print(ds_q[0])\n",
    "\n",
    "ds_qrels = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "df_qrels = pd.DataFrame(ds_qrels['test'])\n",
    "print(df_qrels.head())\n",
    "\n",
    "df_corpus = df_corpus.rename(columns={'_id': 'id'})\n",
    "df_corpus['id'] = df_corpus['id'].astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id title                                               text  query-id  \\\n",
      "0   3        Nie mówię, że nie podoba mi się też pomysł szk...      -1.0   \n",
      "1  31        Tak więc nic nie zapobiega fałszywym ocenom po...      -1.0   \n",
      "2  56        Nigdy nie możesz korzystać z FSA dla indywidua...      -1.0   \n",
      "3  59        Samsung stworzył LCD i inne technologie płaski...      -1.0   \n",
      "4  63        Oto wymagania SEC: Federalne przepisy dotycząc...      -1.0   \n",
      "\n",
      "   score  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n"
     ]
    }
   ],
   "source": [
    "df_joined = pd.merge(df_corpus, df_qrels, left_on='id', right_on='corpus-id', how='left').drop(columns=['corpus-id'])\n",
    "df_joined.fillna({'score': 0, 'query-id': -1}, inplace=True)\n",
    "print(df_joined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'node-1', 'cluster_name': 'my-application-cluster', 'cluster_uuid': '3dsu0IVGSgCf0dqmsaR0NQ', 'version': {'number': '8.15.2', 'build_flavor': 'default', 'build_type': 'docker', 'build_hash': '98adf7bf6bb69b66ab95b761c9e5aadb0bb059a3', 'build_date': '2024-09-19T10:06:03.564235954Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.17.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'You Know, for Search'}\n"
     ]
    }
   ],
   "source": [
    "elasticsearch = Elasticsearch(hosts=[\"http://localhost:9200\"])\n",
    "print(elasticsearch.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 3 & 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'result': 'updated', 'reload_analyzers_details': {'_shards': {'total': 2, 'successful': 1, 'failed': 0}, 'reload_details': [{'index': 'fiqa-pl', 'reloaded_analyzers': ['polish_analyzer_synonyms_included'], 'reloaded_node_ids': ['3FM9a4CGRYK2mHCLDNy-Zw']}]}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_set = [{\"id\": \"synonym-1\", \"synonyms\": \"styczeń, sty, I\"},\n",
    "                {\"id\": \"synonym-2\", \"synonyms\": \"luty, lut, II\"},\n",
    "                {\"id\": \"synonym-3\", \"synonyms\": \"marzec, mar, III\"},\n",
    "                {\"id\": \"synonym-4\", \"synonyms\": \"kwiecień, kwi, IV\"},\n",
    "                {\"id\": \"synonym-5\", \"synonyms\": \"maj, V\"},\n",
    "                {\"id\": \"synonym-6\", \"synonyms\": \"czerwiec, cze, VI\"},\n",
    "                {\"id\": \"synonym-7\", \"synonyms\": \"lipiec, lip, VII\"},\n",
    "                {\"id\": \"synonym-8\", \"synonyms\": \"sierpień, sie, VIII\"},\n",
    "                {\"id\": \"synonym-9\", \"synonyms\": \"wrzesień, wrz, IX\"},\n",
    "                {\"id\": \"synonym-10\", \"synonyms\": \"październik, paź, X\"},\n",
    "                {\"id\": \"synonym-11\", \"synonyms\": \"listopad, lis, XI\"},\n",
    "                {\"id\": \"synonym-12\", \"synonyms\": \"grudzień, gru, XII\"}]\n",
    "\n",
    "elasticsearch.synonyms.put_synonym(id=\"months-synonyms\", synonyms_set=synonyms_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzers = [\"lowercase_analyzer\", \"polish_analyzer_morfologik_included\", \"polish_analyzer_synonyms_included\"]\n",
    "\n",
    "settings = {\n",
    "    \"analysis\": {\n",
    "        \"analyzer\": {\n",
    "            \"lowercase_analyzer\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\"],\n",
    "            },\n",
    "            \"polish_analyzer_morfologik_included\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"morfologik_stem\", \"lowercase\"],\n",
    "            },\n",
    "            \"polish_analyzer_synonyms_included\": {\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"polish_months_filter\", \"lowercase\", \"morfologik_stem\", \"lowercase\"],\n",
    "            },\n",
    "        },\n",
    "        \"filter\": {\n",
    "            \"polish_months_filter\": {\n",
    "                \"type\": \"synonym_graph\",\n",
    "                \"synonyms_set\": \"months-synonyms\",\n",
    "                \"updateable\": True,\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "mappings = {\n",
    "    \"properties\": {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "            \"analyzer\": \"polish_analyzer_morfologik_included\",\n",
    "            \"search_analyzer\": \"polish_analyzer_morfologik_included\",\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def create_index(index_name, settings, mappings):\n",
    "    if elasticsearch.indices.exists(index=index_name):\n",
    "        elasticsearch.indices.delete(index=index_name, ignore_unavailable=True)\n",
    "    elasticsearch.indices.create(index=index_name, mappings=mappings, settings=settings)\n",
    "\n",
    "def create_with_custom_analyzer(index_name, settings, analyzer_name):\n",
    "    mappings = {\n",
    "        \"properties\": {\n",
    "            \"text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": analyzer_name,\n",
    "                \"search_analyzer\": analyzer_name,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    create_index(index_name, settings, mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_index(index_name, ds):\n",
    "    operations = []\n",
    "    for document in ds:\n",
    "        operations.append({\"index\": {\"_index\": index_name}})\n",
    "        operations.append({\"id\": document[\"id\"], \"text\": document[\"text\"]})\n",
    "    elasticsearch.bulk(operations=operations, request_timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re as reg\n",
    "\n",
    "def count_word(amt_of_documents, texts, add_synonyms=False):\n",
    "    name_kwiecien_rule_regex = \"(?:kwie(?:(?:cień)|(?:tni)(?:(?:a(?:(?:mi)|(?:ch)){0,1})|(?:owi)|(?:em)|(?:u)|(?:e)|(?:ów)|(?:om)){0,1}))\"\n",
    "    if add_synonyms:\n",
    "        name_kwiecien_rule_regex = \"(?:\" + name_kwiecien_rule_regex + \"|(?:kwi)|(?:IV))\"\n",
    "    occurences_of_kwiecien = 0\n",
    "    for i in range(amt_of_documents):\n",
    "        occurences_of_kwiecien += len(reg.findall(name_kwiecien_rule_regex, texts[i]['_source']['text'], reg.IGNORECASE))\n",
    "    return occurences_of_kwiecien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 5 & 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_38553/4175109044.py:6: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  elasticsearch.bulk(operations=operations, request_timeout=60)\n"
     ]
    }
   ],
   "source": [
    "create_with_custom_analyzer(\"fiqa-pl\", settings, \"polish_analyzer_morfologik_included\")\n",
    "populate_index(\"fiqa-pl\", df_joined.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "searching_word = \"kwiecień\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents without synonyms:  257\n",
      "matches without synonyms:  354\n"
     ]
    }
   ],
   "source": [
    "result = elasticsearch.search(index=\"fiqa-pl\", size=500, query={\"multi_match\": {\"query\": searching_word,\"analyzer\": \"polish_analyzer_morfologik_included\", \"fields\": [\"text\"]}})\n",
    "amt_of_documents = result['hits']['total']['value']\n",
    "print(\"documents without synonyms: \", amt_of_documents)\n",
    "print(\"matches without synonyms: \", count_word(amt_of_documents, result['hits']['hits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents with synonyms:  306\n",
      "matches with synonyms:  463\n"
     ]
    }
   ],
   "source": [
    "result = elasticsearch.search(index=\"fiqa-pl\", size=500, query={\"multi_match\": {\"query\": searching_word, \"analyzer\": \"polish_analyzer_synonyms_included\", \"fields\": [\"text\"]}})\n",
    "amt_of_documents = result['hits']['total']['value']\n",
    "print(\"documents with synonyms: \", amt_of_documents)\n",
    "print(\"matches with synonyms: \", count_word(amt_of_documents, result['hits']['hits'], True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zad 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute NDCG@5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id:  5993.0  amount of documents:  15\n",
      "query_id:  2348.0  amount of documents:  15\n",
      "query_id:  6005.0  amount of documents:  13\n",
      "query_id:  6131.0  amount of documents:  12\n",
      "query_id:  776.0  amount of documents:  12\n",
      "query_id:  6002.0  amount of documents:  11\n",
      "query_id:  5511.0  amount of documents:  10\n",
      "query_id:  659.0  amount of documents:  10\n",
      "query_id:  10497.0  amount of documents:  10\n",
      "query_id:  3909.0  amount of documents:  10\n",
      "query_id:  4409.0  amount of documents:  10\n"
     ]
    }
   ],
   "source": [
    "corpus_query_mapping = {} # query_id -> [corpus_ids]\n",
    "\n",
    "for i, row in df_joined.iterrows():\n",
    "    if row['query-id'] == -1:\n",
    "        continue\n",
    "    if row['query-id'] not in corpus_query_mapping:\n",
    "        corpus_query_mapping[row['query-id']] = []\n",
    "    corpus_query_mapping[row['query-id']].append(row['id'])\n",
    "\n",
    "ranking = []\n",
    "maxi = 0\n",
    "id_maxi = -1\n",
    "for query_id, corpus_ids in corpus_query_mapping.items():\n",
    "    ranking.append((corpus_ids, query_id))\n",
    "    if len(corpus_ids) > maxi:\n",
    "        maxi = len(corpus_ids)\n",
    "        id_maxi = query_id\n",
    "\n",
    "ranking = sorted(ranking, key=lambda x: len(x[0]), reverse=True)\n",
    "best_queries = []\n",
    "for idx, (corpus_ids, query_id) in enumerate(ranking):\n",
    "    print(\"query_id: \", query_id, \" amount of documents: \", len(corpus_ids))\n",
    "    best_queries.append(int(query_id))\n",
    "    if idx == 10: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg(result, searching_query_id):\n",
    "    amount_of_one = len(corpus_query_mapping[searching_query_id])\n",
    "\n",
    "    DCG = 0\n",
    "    for idx, hit in enumerate(result['hits']['hits']):\n",
    "        gain = 0\n",
    "        corpus_id = hit['_source']['id']\n",
    "        if corpus_id in corpus_query_mapping[searching_query_id]:\n",
    "            gain = 1\n",
    "        DCG += gain / np.log2(idx + 2)\n",
    "\n",
    "    IDCG = 0\n",
    "    for idx in range(amount_of_one):\n",
    "        IDCG += 1 / np.log2(idx + 2)\n",
    "\n",
    "    if IDCG == 0:\n",
    "        nDCG = 0\n",
    "    else: \n",
    "        nDCG = DCG / IDCG\n",
    "    print(\"nDCG: \", nDCG)\n",
    "    return nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_analyzers(searching_query_id):\n",
    "    searching_word = ds_q[searching_query_id]['text']\n",
    "    # print(\"searching word: \", searching_word)\n",
    "    result = elasticsearch.search(index=\"fiqa-pl\", size=10000, query={\"multi_match\": {\"query\": searching_word, \"analyzer\": \"polish_analyzer_morfologik_included\", \"fields\": [\"text\"]}})\n",
    "    amt_of_documents = result['hits']['total']['value']\n",
    "    print(\"documents without synonyms: \", amt_of_documents)\n",
    "    ndcg1 = calculate_ndcg(result, searching_query_id)\n",
    "\n",
    "    print(\" \")\n",
    "\n",
    "    result = elasticsearch.search(index=\"fiqa-pl\", size=10000, query={\"multi_match\": {\"query\": searching_word, \"analyzer\": \"polish_analyzer_synonyms_included\", \"fields\": [\"text\"]}})\n",
    "    amt_of_documents = result['hits']['total']['value']\n",
    "    print(\"documents with synonyms: \", amt_of_documents)\n",
    "    ndcg2 = calculate_ndcg(result, searching_query_id)\n",
    "    print(\" \")\n",
    "\n",
    "    result = elasticsearch.search(index=\"fiqa-pl\", size=10000, query={\"multi_match\": {\"query\": searching_word, \"analyzer\": \"lowercase_analyzer\", \"fields\": [\"text\"]}})\n",
    "    amt_of_documents = result['hits']['total']['value']\n",
    "    print(\"documents without synonyms and lemmatization: \", amt_of_documents)\n",
    "    ndcg3 = calculate_ndcg(result, searching_query_id)\n",
    "    print(\" \")\n",
    "\n",
    "    result = elasticsearch.search(index=\"fiqa-pl\", size=10000, query={\"multi_match\": {\"query\": searching_word, \"analyzer\": \"polish_analyzer_synonyms_included\", \"fields\": [\"text\"]}})\n",
    "    amt_of_documents = result['hits']['total']['value']\n",
    "    print(\"documents with synonyms and lemmatization: \", amt_of_documents)\n",
    "    ndcg4 = calculate_ndcg(result, searching_query_id)\n",
    "    print(\" \")\n",
    "    return ndcg1, ndcg2, ndcg3, ndcg4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id:  5993\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.026717807395236236\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.026717807395236236\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.0\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.026717807395236236\n",
      " \n",
      "query_id:  2348\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.1359527185551999\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.1359527185551999\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.04089654341096002\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.1359527185551999\n",
      " \n",
      "query_id:  6005\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.02964862909292818\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.02964862909292818\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.03154922871116617\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.02964862909292818\n",
      " \n",
      "query_id:  6131\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.029969257893291325\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.029969257893291325\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.0\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.029969257893291325\n",
      " \n",
      "query_id:  776\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.04750337391408004\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.04750337391408004\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.05676468628431504\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.04750337391408004\n",
      " \n",
      "query_id:  6002\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.10327540330649436\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.10327540330649436\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.05336889585219605\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.10327540330649436\n",
      " \n",
      "query_id:  5511\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.01777501861425866\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.01777501861425866\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.05318913535953133\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.01777501861425866\n",
      " \n",
      "query_id:  659\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.07097387059849962\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.07097387059849962\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.038247191374833255\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.07097387059849962\n",
      " \n",
      "query_id:  3909\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.07542605901801526\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.07542605901801526\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.03522159882499243\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.07542605901801526\n",
      " \n",
      "query_id:  4409\n",
      "documents without synonyms:  10000\n",
      "nDCG:  0.09191384786308558\n",
      " \n",
      "documents with synonyms:  10000\n",
      "nDCG:  0.09191384786308558\n",
      " \n",
      "documents without synonyms and lemmatization:  10000\n",
      "nDCG:  0.03889953693330713\n",
      " \n",
      "documents with synonyms and lemmatization:  10000\n",
      "nDCG:  0.09191384786308558\n",
      " \n",
      "nDCG average:  0.05589011938761422\n"
     ]
    }
   ],
   "source": [
    "nDCG_collected = []\n",
    "for query_id in best_queries:\n",
    "    if query_id > 6648:\n",
    "        continue\n",
    "    print(\"query_id: \", query_id)\n",
    "    nDCG1, nDCG2, nDCG3, nDCG4 = compare_analyzers(query_id)\n",
    "    nDCG_collected.append(nDCG1)\n",
    "    nDCG_collected.append(nDCG2)\n",
    "    nDCG_collected.append(nDCG3)\n",
    "    nDCG_collected.append(nDCG4)\n",
    "\n",
    "print(\"nDCG average: \", np.mean(nDCG_collected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Zad 11\n",
    "Find three questions from the test subset with the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_for_relevant_document_at_given_position(position):\n",
    "    if position > 10000:\n",
    "        position = 10000\n",
    "    for idx, query in enumerate(ds_q):\n",
    "        result = elasticsearch.search(index=\"fiqa-pl\", size=10000, query={\"multi_match\": {\"query\": query['text'], \"analyzer\": \"polish_analyzer_synonyms_included\", \"fields\": [\"text\"]}})\n",
    "        \n",
    "        if (idx/len(ds_q))*100 % 100 == 0:\n",
    "            print(idx)\n",
    "\n",
    "        lower_sum = []\n",
    "        for idx, hit in enumerate(result[\"hits\"][\"hits\"]):\n",
    "            gain = 0\n",
    "            corpus_id = result['hits']['hits'][idx]['_source']['id']\n",
    "            if query['_id'] in corpus_query_mapping and corpus_id in corpus_query_mapping[query['_id']]:\n",
    "                gain = 1\n",
    "            if idx < position:\n",
    "                lower_sum.append(gain)\n",
    "        \n",
    "        gain = 0\n",
    "        corpus_id = result['hits']['hits'][position]['_source']['id']\n",
    "        if query['_id'] in corpus_query_mapping and corpus_id in corpus_query_mapping[query['_id']]:\n",
    "            gain = 1\n",
    "\n",
    "        if position < len(result[\"hits\"][\"hits\"]) and gain == 1 and sum(lower_sum) == 0:\n",
    "        \n",
    "            print(\"Searched query: \", query['text'], \" found at: \", idx)\n",
    "            break\n",
    "        if position >= len(ds_q) and sum(lower_sum) == 0:\n",
    "            print(\"Searched query: \", query['text'], \" found at: \", idx)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the relevant document is returned by ES at position 1,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_for_relevant_document_at_given_position(1) # took too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the relevant document is returned by ES at position 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_for_relevant_document_at_given_position(4) # took too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look_for_relevant_document_at_given_position(5) # took too long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the relevant document is returned by ES is not found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Searched query:  Co jest uważane za wydatek służbowy w podróży służbowej?  found at:  9999\n"
     ]
    }
   ],
   "source": [
    "look_for_relevant_document_at_given_position(len(ds_q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What are the strengths and weaknesses of regular expressions versus full text search regarding processing of text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regular expressions give more control over searching but are much more complex. In the presented example FTS found fewer results than regExp in previous labs. But FTS is more convenient and also gives satisfying results, almost the same as regexp. FTS has a good ecosystem and a lot of features that facilitate and improve data processing. In most cases, FTS will be enough, but for precise outcomes, we should probably use both methods to ensure the best results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Can an LLM be applied in the context of searching for documents? Justify your answer, excluding the obvious observation that an LLM can be used to formulate the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM has a tremendous impact on searching. As far as LLM can recognize the text LLM can change, enrich or interpret it. LLM can take the query and try to understand its context, by generating more sentences or guessing the most probable connections and asking the search engine again. Also if the text can be represented in another form or expression then the generative model can translate it in such a way e.g. to a different language.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-labs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
