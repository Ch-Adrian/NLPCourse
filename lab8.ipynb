{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural search for question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the documentation of the [document store](https://docs.haystack.deepset.ai/docs/document_store) and\n",
    "   the [retriever](https://docs.haystack.deepset.ai/docs/retriever) in the \n",
    "   [Haystack framework](https://haystack.deepset.ai/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Install Haystack framework (e.g. with `pip install 'farm-haystack[all]'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Configure a document store based on Faiss supported by multilingual E5 model:\n",
    "   1. For Faiss use [multilingual E5](https://huggingface.co/intfloat/multilingual-e5-base) or [silver retriever base](https://huggingface.co/ipipan/silver-retriever-base-v1) encoder.\n",
    "   3. **Warning:** If you use E5, make sure to [properly configure](https://github.com/deepset-ai/haystack/issues/5242) the store.\n",
    "   4. In the case you have problems using Faiss, you can use `InMemoryDocumentStore`, but this will require to re-index\n",
    "      all documents each time the script is run, which is time consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from haystack import Pipeline\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever, PromptNode\n",
    "from haystack.document_stores import InMemoryDocumentStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION WARNINGS:\n",
      "                1. delete_all_documents() method is deprecated, please use delete_documents method\n",
      "                For more details, please refer to the issue: https://github.com/deepset-ai/haystack/issues/1045\n",
      "                \n"
     ]
    }
   ],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.nodes import EmbeddingRetriever, PromptNode\n",
    "\n",
    "document_store = FAISSDocumentStore(faiss_index_factory_str=\"Flat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = EmbeddingRetriever(\n",
    "    document_store=document_store,\n",
    "    embedding_model=\"intfloat/multilingual-e5-base\",\n",
    "    model_format=\"transformers\",\n",
    "    pooling_strategy=\"reduce_mean\",\n",
    "    top_k=5,\n",
    "    max_seq_len=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "document_store_in_memory = InMemoryDocumentStore(similarity=\"cosine\", embedding_dim=768)\n",
    "\n",
    "retriever_for_in_memory = EmbeddingRetriever(\n",
    "    document_store=document_store_in_memory,\n",
    "    embedding_model=\"intfloat/multilingual-e5-base\",\n",
    "    model_format=\"transformers\",\n",
    "    pooling_strategy=\"reduce_mean\",\n",
    "    top_k=5,\n",
    "    max_seq_len=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating Embedding: 0 docs [00:00, ? docs/s]\n"
     ]
    }
   ],
   "source": [
    "document_store_in_memory.update_embeddings(retriever_for_in_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "print(retriever.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Load the documents (passages) from the FiQA corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id title                                               text\n",
      "0   3        Nie mówię, że nie podoba mi się też pomysł szk...\n",
      "1  31        Tak więc nic nie zapobiega fałszywym ocenom po...\n",
      "2  56        Nigdy nie możesz korzystać z FSA dla indywidua...\n",
      "3  59        Samsung stworzył LCD i inne technologie płaski...\n",
      "4  63        Oto wymagania SEC: Federalne przepisy dotycząc...\n",
      "   id title                                               text\n",
      "0   0        Co jest uważane za wydatek służbowy w podróży ...\n",
      "1   4        Wydatki służbowe - ubezpieczenie samochodu pod...\n",
      "2   5                        Rozpoczęcie nowego biznesu online\n",
      "3   6           „Dzień roboczy” i „termin płatności” rachunków\n",
      "4   7        Nowy właściciel firmy – Jak działają podatki d...\n",
      "   query-id  corpus-id  score\n",
      "0         0      18850      1\n",
      "1         4     196463      1\n",
      "2         5      69306      1\n",
      "3         6     560251      1\n",
      "4         6     188530      1\n",
      "   query-id  corpus-id  score\n",
      "0         8     566392      1\n",
      "1         8      65404      1\n",
      "2        15     325273      1\n",
      "3        18      88124      1\n",
      "4        26     285255      1\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def modify_df(df, labels):\n",
    "    for label in labels:\n",
    "        if label[0] == '_':\n",
    "            df = df.rename(columns={label: label[1:]})\n",
    "            label = label[1:]\n",
    "        df[label] = df[label].astype(np.int64)\n",
    "    return df\n",
    "\n",
    "ds = load_dataset(\"clarin-knext/fiqa-pl\", \"corpus\")\n",
    "df_corpus = pd.DataFrame(ds['corpus'])\n",
    "df_corpus = modify_df(df_corpus, ['_id'])\n",
    "print(df_corpus.head())\n",
    "\n",
    "ds_q = load_dataset(\"clarin-knext/fiqa-pl\", \"queries\")\n",
    "ds_q = pd.DataFrame(ds_q['queries'])\n",
    "ds_q = modify_df(ds_q, ['_id'])\n",
    "print(ds_q.head())\n",
    "\n",
    "ds_qrels = load_dataset(\"clarin-knext/fiqa-pl-qrels\")\n",
    "df_qrels = pd.DataFrame(ds_qrels['train'])\n",
    "df_qrels_test = pd.DataFrame(ds_qrels['test'])\n",
    "df_qrels = modify_df(df_qrels, ['query-id', 'corpus-id', 'score'])\n",
    "df_qrels_test = modify_df(df_qrels_test, ['query-id', 'corpus-id', 'score'])\n",
    "print(df_qrels.head())\n",
    "print(df_qrels_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dlaczego ktokolwiek miałby chcieć najpierw spłacić swoje długi w inny sposób niż „najwyższe odsetki”?\n"
     ]
    }
   ],
   "source": [
    "ds_q_mapper = dict()\n",
    "ds_corpus_mapper = dict()\n",
    "\n",
    "for idx, query in ds_q.iterrows():\n",
    "    ds_q_mapper[query['id']] = query['text']\n",
    "\n",
    "for idx, corpus in df_corpus.iterrows():\n",
    "    ds_corpus_mapper[corpus['id']] = corpus['text']\n",
    "\n",
    "\n",
    "print(ds_q_mapper[5993])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id title                                               text  query-id  \\\n",
      "0   3        Nie mówię, że nie podoba mi się też pomysł szk...        -1   \n",
      "1  31        Tak więc nic nie zapobiega fałszywym ocenom po...        -1   \n",
      "2  56        Nigdy nie możesz korzystać z FSA dla indywidua...        -1   \n",
      "3  59        Samsung stworzył LCD i inne technologie płaski...        -1   \n",
      "4  63        Oto wymagania SEC: Federalne przepisy dotycząc...        -1   \n",
      "\n",
      "   score  \n",
      "0    0.0  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n"
     ]
    }
   ],
   "source": [
    "df_joined_test = pd.merge(df_corpus, df_qrels_test, left_on='id', right_on='corpus-id', how='left').drop(columns=['corpus-id'])\n",
    "df_joined_test.fillna({'score': 0, 'query-id': -1}, inplace=True)\n",
    "df_joined_test['query-id'] = df_joined_test['query-id'].astype(np.int64)\n",
    "print(df_joined_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "passages = []\n",
    "for idx, data in df_corpus.iterrows():\n",
    "    id = data['id']\n",
    "    title = data['title']\n",
    "    txt = data['text']\n",
    "    passages.append({'content': f\"passage: {txt}\", 'meta': {'id': data['id']}})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'passage: Nie mówię, że nie podoba mi się też pomysł szkolenia w miejscu pracy, ale nie możesz oczekiwać, że firma to zrobi. Szkolenie pracowników to nie ich praca – oni tworzą oprogramowanie. Być może systemy edukacyjne w Stanach Zjednoczonych (lub ich studenci) powinny trochę martwić się o zdobycie umiejętności rynkowych w zamian za ich ogromne inwestycje w edukację, zamiast wychodzić z tysiącami zadłużonych studentów i narzekać, że nie są do niczego wykwalifikowani.', 'meta': {'id': 3}}, {'content': 'passage: Tak więc nic nie zapobiega fałszywym ocenom poza dodatkową kontrolą ze strony rynku/inwestorów, ale istnieją pewne nowsze kontrole, które uniemożliwiają instytucjom korzystanie z nich. W ramach DFA banki nie mogą już polegać wyłącznie na ratingach kredytowych jako należytej staranności przy zakupie instrumentu finansowego, więc to jest plus. Intencją jest to, że jeśli instytucje finansowe wykonują swoją własną pracę, to *być może* dojdą do wniosku, że określony CDO jest śmieciem, czy nie. Edycja: wprowadzenie', 'meta': {'id': 31}}, {'content': 'passage: Nigdy nie możesz korzystać z FSA dla indywidualnych składek na ubezpieczenie zdrowotne. Co więcej, sponsorzy planu FSA mogą ograniczyć to, co chcą zwrócić. Chociaż nie możesz użyć FSA zdrowia do składek, wcześniej można było użyć planu kafeteryjnego 125 do opłacania składek, ale musiały to być oddzielne wybory od FSA zdrowia. Jednak zgodnie z N. 2013-54 nawet korzystanie z planu kafeteryjnego do opłacania indywidualnych składek jest skutecznie zabronione.', 'meta': {'id': 56}}, {'content': 'passage: Samsung stworzył LCD i inne technologie płaskiego ekranu, takie jak OLED. kilka lat temu każdy płaski ekran pochodził z fabryk Samsunga i był wymieniany. Myślę, że ekran 21 Hanns, na który teraz patrzę, to Samsung i ma zaledwie kilka lat. Samsung wydaje się być dobrą firmą.', 'meta': {'id': 59}}]\n"
     ]
    }
   ],
   "source": [
    "print(passages[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION WARNINGS:\n",
      "                1. delete_all_documents() method is deprecated, please use delete_documents method\n",
      "                For more details, please refer to the issue: https://github.com/deepset-ai/haystack/issues/1045\n",
      "                \n",
      "Writing Documents: 60000it [01:51, 539.68it/s]                           \n"
     ]
    }
   ],
   "source": [
    "document_store.delete_all_documents()\n",
    "document_store.write_documents(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store_in_memory.delete_documents()\n",
    "document_store_in_memory.write_documents(passages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.34 Batches/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [58:21<00:00, 11.19s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [57:51<00:00, 11.09s/ Batches]/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [57:57<00:00, 11.11s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [57:52<00:00, 11.09s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [59:55<00:00, 11.49s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 238/238 [45:58<00:00, 11.59s/ Batches]/s]  \n",
      "Documents Processed: 60000 docs [5:38:51,  2.95 docs/s]                        \n"
     ]
    }
   ],
   "source": [
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 313/313 [59:37<00:00, 11.43s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [59:23<00:00, 11.38s/ Batches]/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [59:27<00:00, 11.40s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [59:19<00:00, 11.37s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 313/313 [59:38<00:00, 11.43s/ Batches]cs/s]\n",
      "Inferencing Samples: 100%|██████████| 238/238 [45:18<00:00, 11.42s/ Batches]/s]  \n",
      "Documents Processed: 60000 docs [5:43:10,  2.91 docs/s]                        \n"
     ]
    }
   ],
   "source": [
    "document_store_in_memory.update_embeddings(retriever_for_in_memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6m38s 1000 samples in 256 batchsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store.save(index_path=\"./data/index2/document_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.document_stores.faiss.FAISSDocumentStore at 0x7f9fd4434370>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_store.load(index_path=\"./data/index2/document_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.81 Batches/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Document: {'content': 'passage: Dla Maca to zdecydowanie iFinance.', 'content_type': 'text', 'score': 0.9072425449685203, 'meta': {'id': 281322, 'vector_id': '22006'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '6bcefb03b1b61c71fb5364758f5ee8d1'}>,\n",
       " <Document: {'content': 'passage: ', 'content_type': 'text', 'score': 0.9049649822622359, 'meta': {'id': 597929, 'vector_id': '19650'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '61f526150704962cb6b420e009e00ac9'}>,\n",
       " <Document: {'content': 'passage: Tak, możesz. nazywa się Odd Lot', 'content_type': 'text', 'score': 0.9029244803001298, 'meta': {'id': 194404, 'vector_id': '47622'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'd68b76e74408bc6055d29a5f7145d1de'}>,\n",
       " <Document: {'content': 'passage: 2 rzeczy:', 'content_type': 'text', 'score': 0.8984933453295506, 'meta': {'id': 339473, 'vector_id': '44576'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'ca27e31718841de4a9c5e3f96b62d78b'}>,\n",
       " <Document: {'content': 'passage: TAk. Krótkie i słodkie.', 'content_type': 'text', 'score': 0.8960346020136766, 'meta': {'id': 136784, 'vector_id': '45013'}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'cbf337f30758f371202292c923b6d41'}>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.retrieve(query=\"What is haystack?\", top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(document_store.get_document_by_id(id=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Use the set of questions and the scorings defined in this corpus, to compute NDCG@5 for the dense retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id:  5993  amount of documents:  15 documents:  [5827, 55084, 63501, 63690, 94373, 128574, 160193, 224918, 230215, 272866, 287571, 352638, 367375, 426120, 431212]\n",
      "query_id:  2348  amount of documents:  15 documents:  [134864, 146441, 211622, 211867, 247486, 265874, 268261, 306430, 352271, 381757, 410166, 447619, 474234, 543714, 566573]\n",
      "query_id:  6005  amount of documents:  13 documents:  [73310, 135415, 149500, 176498, 270856, 345895, 384626, 390689, 414288, 414534, 478457, 507544, 572272]\n",
      "query_id:  6131  amount of documents:  12 documents:  [2460, 170204, 218088, 235452, 252534, 258465, 326094, 334111, 365263, 368806, 381720, 416679]\n",
      "query_id:  776  amount of documents:  12 documents:  [10440, 124027, 127263, 220127, 332373, 467044, 496899, 583640, 591516, 592680, 597247, 597880]\n",
      "query_id:  6002  amount of documents:  11 documents:  [34389, 154181, 233472, 273501, 359862, 390642, 391819, 404605, 516848, 519346, 593434]\n",
      "query_id:  5511  amount of documents:  10 documents:  [12746, 51873, 107898, 114303, 169893, 278699, 383193, 478426, 529123, 560325]\n",
      "query_id:  659  amount of documents:  10 documents:  [13139, 120279, 168796, 230908, 235925, 264297, 365240, 439467, 449079, 584685]\n",
      "query_id:  10497  amount of documents:  10 documents:  [31483, 34913, 71898, 147765, 159880, 196423, 304284, 398622, 445230, 575729]\n",
      "query_id:  3909  amount of documents:  10 documents:  [61586, 193459, 200690, 245616, 312248, 353028, 374400, 404352, 404356, 514003]\n",
      "query_id:  4409  amount of documents:  10 documents:  [97925, 100306, 102088, 102326, 115066, 147439, 245903, 360682, 426676, 499128]\n"
     ]
    }
   ],
   "source": [
    "corpus_query_mapping = {} # query_id -> [corpus_ids]\n",
    "\n",
    "for i, row in df_joined_test.iterrows():\n",
    "    if row['query-id'] == -1:\n",
    "        continue\n",
    "    if row['query-id'] not in corpus_query_mapping:\n",
    "        corpus_query_mapping[int(row['query-id'])] = []\n",
    "    corpus_query_mapping[int(row['query-id'])].append(row['id'])\n",
    "\n",
    "ranking = []\n",
    "maxi = 0\n",
    "id_maxi = -1\n",
    "for query_id, corpus_ids in corpus_query_mapping.items():\n",
    "    ranking.append((corpus_ids, query_id))\n",
    "    if len(corpus_ids) > maxi:\n",
    "        maxi = len(corpus_ids)\n",
    "        id_maxi = query_id\n",
    "\n",
    "ranking = sorted(ranking, key=lambda x: len(x[0]), reverse=True)\n",
    "best_queries = []\n",
    "for idx, (corpus_ids, query_id) in enumerate(ranking):\n",
    "    print(\"query_id: \", query_id, \" amount of documents: \", len(corpus_ids), \"documents: \", corpus_ids)\n",
    "    best_queries.append(int(query_id))\n",
    "    if idx == 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ndcg_for_QA(result, searching_query_id, n):\n",
    "    amount_of_one = len(corpus_query_mapping[int(searching_query_id)])\n",
    "\n",
    "    DCG = 0\n",
    "    for idx, answer in enumerate(result):\n",
    "        # print(answer.meta)\n",
    "        gain = 0\n",
    "        corpus_id = answer.meta['id']\n",
    "\n",
    "        if corpus_id in corpus_query_mapping[int(searching_query_id)]:\n",
    "            gain = 1\n",
    "\n",
    "        DCG += gain / np.log2(idx + 2)\n",
    "        if idx == n: break\n",
    "\n",
    "    IDCG = 0\n",
    "    for idx in range(min(amount_of_one, n)):\n",
    "        IDCG += 1 / np.log2(idx + 2)\n",
    "\n",
    "    if IDCG == 0:\n",
    "        nDCG = 0\n",
    "    else: \n",
    "        nDCG = DCG / IDCG\n",
    "\n",
    "    return nDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.08 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5993, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.86 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2348, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.33 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6005, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.25 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6131, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.94 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 776, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.14 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6002, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.84 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5511, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.28 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 659, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.24 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 10497, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 3909, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.35 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 4409, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.30 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 3724, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.72 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2075, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.18 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5951, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.30 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2296, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.00 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2204, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.33 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 8974, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.36 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2685, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.18 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 11039, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.15 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2376, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.79 Batches/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6221, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "encoded_query_dict = {} # query_id -> encoded_query\n",
    "n = 5\n",
    "scores_5 = {}\n",
    "scores_10 = {}\n",
    "for id, (coupus_id, query_id) in enumerate(ranking):\n",
    "    result = retriever.retrieve(query=\"query: \"+ds_q_mapper[query_id], top_k=n)\n",
    "    ndcg_5 = calculate_ndcg_for_QA(result, query_id, 5)\n",
    "    ndcg_10 = calculate_ndcg_for_QA(result, query_id, 10)\n",
    "    scores_5[query_id] = ndcg_5\n",
    "    scores_10[query_id] = ndcg_10\n",
    "    print(f\"query_id: {query_id}, ndcg5: {ndcg_5}, ndcg10: {ndcg_10}\")\n",
    "    if id == 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.77 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5993, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.91 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2348, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.05 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6005, ndcg5: 0.48522855511632257, ndcg10: 0.31488013066763093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.96 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6131, ndcg5: 0.16958010263680806, ndcg10: 0.11004588314904008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.47 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 776, ndcg5: 0.3391602052736161, ndcg10: 0.22009176629808017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.78 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6002, ndcg5: 0.6548086577531307, ndcg10: 0.424926013816671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5511, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 659, ndcg5: 0.5087403079104241, ndcg10: 0.33013764944712026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.99 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 10497, ndcg5: 0.6992148198508501, ndcg10: 0.4537425745411855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.40 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 3909, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.89 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 4409, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.67 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 3724, ndcg5: 0.3391602052736161, ndcg10: 0.22009176629808017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.06 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2075, ndcg5: 0.3391602052736161, ndcg10: 0.23504554941448536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.07 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 5951, ndcg5: 0.3391602052736161, ndcg10: 0.23504554941448536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.74 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2296, ndcg5: 0.5531464700081437, ndcg10: 0.38334277998463445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.35 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2204, ndcg5: 0.7227265726449519, ndcg10: 0.5390031312763821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  1.91 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 8974, ndcg5: 0.3391602052736161, ndcg10: 0.2529427027676571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.01 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2685, ndcg5: 0.13120507751234178, ndcg10: 0.09785159463516042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 11039, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.46 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 2376, ndcg5: 0.8687949224876582, ndcg10: 0.647939623894138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:00<00:00,  2.44 Batches/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_id: 6221, ndcg5: 0.0, ndcg10: 0.0\n"
     ]
    }
   ],
   "source": [
    "encoded_query_dict = {} # query_id -> encoded_query\n",
    "n = 5\n",
    "scores_5 = {}\n",
    "scores_10 = {}\n",
    "for id, (coupus_id, query_id) in enumerate(ranking):\n",
    "    result = retriever_for_in_memory.retrieve(query=\"query: \"+ds_q_mapper[query_id], top_k=n)\n",
    "    ndcg_5 = calculate_ndcg_for_QA(result, query_id, 5)\n",
    "    ndcg_10 = calculate_ndcg_for_QA(result, query_id, 10)\n",
    "    scores_5[query_id] = ndcg_5\n",
    "    scores_10[query_id] = ndcg_10\n",
    "    print(f\"query_id: {query_id}, ndcg5: {ndcg_5}, ndcg10: {ndcg_10}\")\n",
    "    if id == 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Compare the NDCG score from this exercise with the score from [lab 2](2-fts.md) and from [lab 6](6-classification.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_from_lab2 = {\n",
    "    \"5993\": 0.0267,\n",
    "    \"2348\": 0.13595,\n",
    "    \"6005\": 0.029648,\n",
    "    \"6131\": 0.029969,\n",
    "    \"776\": 0.04750,\n",
    "    \"6002\": 0.103275,\n",
    "    \"5511\": 0.017775,\n",
    "    \"659\": 0.070973,\n",
    "    \"3909\": 0.075426,\n",
    "    \"4409\": 0.091913,\n",
    "}\n",
    "\n",
    "scores_from_lab6 = {\n",
    "    \"5993\": [0.2837, 0.2048],\n",
    "    \"2348\": [0.2048, 0.3301],\n",
    "    \"6005\": [0.2719, 0.3882],\n",
    "    \"6131\": [0.2201, 0.2201],\n",
    "    \"776\": [0.2837, 0.1952],\n",
    "    \"6002\": [0.6236, 0.4331],\n",
    "    \"5511\": [0, 0],\n",
    "    \"659\": [0, 0],\n",
    "    \"3909\": [0, 0],\n",
    "    \"4409\": [0, 0],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "nDCG_results_table = PrettyTable(['ndcg5', 'ndcg10', 'lab2', 'lab6_FTS', 'lab6_Reranker'])\n",
    "\n",
    "for idx, ((_,ndcg5), (_,ndcg10), (_,lab2), (_,lab6)) in enumerate(zip(scores_5.items(), scores_10.items(), scores_from_lab2.items(), scores_from_lab6.items())):\n",
    "    nDCG_results_table.add_row([round(ndcg5, 4), round(ndcg10, 4), lab2, lab6[0], lab6[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+----------+---------------+\n",
      "| ndcg5  | ndcg10 |   lab2   | lab6_FTS | lab6_Reranker |\n",
      "+--------+--------+----------+----------+---------------+\n",
      "|  0.0   |  0.0   |  0.0267  |  0.2837  |     0.2048    |\n",
      "|  0.0   |  0.0   | 0.13595  |  0.2048  |     0.3301    |\n",
      "| 0.4852 | 0.3149 | 0.029648 |  0.2719  |     0.3882    |\n",
      "| 0.1696 |  0.11  | 0.029969 |  0.2201  |     0.2201    |\n",
      "| 0.3392 | 0.2201 |  0.0475  |  0.2837  |     0.1952    |\n",
      "| 0.6548 | 0.4249 | 0.103275 |  0.6236  |     0.4331    |\n",
      "|  0.0   |  0.0   | 0.017775 |    0     |       0       |\n",
      "| 0.5087 | 0.3301 | 0.070973 |    0     |       0       |\n",
      "| 0.6992 | 0.4537 | 0.075426 |    0     |       0       |\n",
      "|  0.0   |  0.0   | 0.091913 |    0     |       0       |\n",
      "+--------+--------+----------+----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "print(nDCG_results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. **Bonus** (+2p) Combine dense retrieval with classification model from [lab 6](6-classification.md) to implement a two-step\n",
    "   retrieval. Compute NDCG@5 for this combined model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. **Bonus** (+2p) Use a different dense encoder, e.g. [E5 large](https://huggingface.co/intfloat/multilingual-e5-large) or [Polish Roberta Base](https://huggingface.co/sdadas/mmlw-retrieval-roberta-base) and compute NDCG@5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions (2 points)\n",
    "\n",
    "Disclaimer:\n",
    "Faiss document store didn't work, so I used InMemoryDocumentStore.\n",
    "\n",
    "1. Which of the methods: lexical match (e.g. ElasticSearch) or dense representation works better?\\\n",
    "    **Dense representation works better. Sometimes FTS or Reranker performs well, but for dense representation the results achieved higher figures.**\n",
    "2. Which of the methods is faster?\\\n",
    "    **If we consider preprocessing Elastic Search is faster, but process of searching answer is quite the same for both.**\n",
    "3. Try to determine the other pros and cons of using lexical search and dense document retrieval models.\\\n",
    "    **Dense retrieval model:\\\n",
    "        pros:\\\n",
    "        - it incorporates meaning in searching, so it is able to search more advanced language constructions with paraphrases words\\\n",
    "        cons:\\\n",
    "        - computationally intensive\\\n",
    "        - can omit keywords\\\n",
    "        - require fine-tuning and specified training\\\n",
    "      Lexical search:\\\n",
    "        pros:\\\n",
    "        - looks for exact match of words and some of phrases requires that\\\n",
    "        - relatively simple and fast\\\n",
    "        cons:\\\n",
    "        - difficult to handle synonyms, misspellings paraphrases\\\n",
    "        - doesn't take meaning of words into account and it leads to useless results**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
